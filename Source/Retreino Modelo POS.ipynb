{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function\n",
    "\n",
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.symbols import ADJ, ADV, INTJ, NOUN, PROPN, VERB, ADP, AUX, CCONJ,DET, NUM, PRON, SCONJ, PUNCT, SYM, X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA = []\n",
    "\n",
    "with open('Sentencas.csv', 'r') as data_file:\n",
    "    lines = data_file.readlines()\n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        tokens = lines[i].split(\"***\")\n",
    "        i += 1\n",
    "        tags = lines[i].split(\"***\")\n",
    "        i += 3\n",
    "        TRAIN_DATA.append((\"{}\".format(' '.join(tokens[:-1])), {\"tags\": tags[:-1]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130\n",
      "('Ela vive postando selfie . Quer biscoito essa d aí', {'tags': ['PRON', 'VERB', 'VERB', 'NOUN', 'PUNCT', 'VERB', 'NOUN', 'ADP', 'ADP', 'ADV']})\n",
      "('to bem tb kkk vou passar pra minha mãe', {'tags': ['ADP', 'ADV', 'X', 'X', 'AUX', 'VERB', 'DET', 'DET', 'NOUN']})\n"
     ]
    }
   ],
   "source": [
    "TRAIN_DATA[:2]\n",
    "random.shuffle(TRAIN_DATA)\n",
    "print(len(TRAIN_DATA))\n",
    "TEST_DATA = TRAIN_DATA[:43]\n",
    "TRAIN_DATA = TRAIN_DATA[43:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0\n",
      "Error on   {'tags': []}\n",
      "Error on   {'tags': []}\n",
      "Error on  Hownt , que fofo  *- {'tags': ['NOUN', 'PUNCT', 'PRON', 'VERB', 'PUNCT', 'PUNCT']}\n",
      "Epoch  1\n",
      "Error on   {'tags': []}\n",
      "Error on   {'tags': []}\n",
      "Error on  Hownt , que fofo  *- {'tags': ['NOUN', 'PUNCT', 'PRON', 'VERB', 'PUNCT', 'PUNCT']}\n",
      "Epoch  2\n",
      "Error on   {'tags': []}\n",
      "Error on  Hownt , que fofo  *- {'tags': ['NOUN', 'PUNCT', 'PRON', 'VERB', 'PUNCT', 'PUNCT']}\n",
      "Error on   {'tags': []}\n",
      "Epoch  3\n",
      "Error on  Hownt , que fofo  *- {'tags': ['NOUN', 'PUNCT', 'PRON', 'VERB', 'PUNCT', 'PUNCT']}\n",
      "Error on   {'tags': []}\n",
      "Error on   {'tags': []}\n",
      "Epoch  4\n",
      "Error on   {'tags': []}\n",
      "Error on   {'tags': []}\n",
      "Error on  Hownt , que fofo  *- {'tags': ['NOUN', 'PUNCT', 'PRON', 'VERB', 'PUNCT', 'PUNCT']}\n",
      "Epoch  5\n",
      "Error on   {'tags': []}\n",
      "Error on   {'tags': []}\n",
      "Error on  Hownt , que fofo  *- {'tags': ['NOUN', 'PUNCT', 'PRON', 'VERB', 'PUNCT', 'PUNCT']}\n",
      "Epoch  6\n",
      "Error on   {'tags': []}\n",
      "Error on  Hownt , que fofo  *- {'tags': ['NOUN', 'PUNCT', 'PRON', 'VERB', 'PUNCT', 'PUNCT']}\n",
      "Error on   {'tags': []}\n",
      "Epoch  7\n",
      "Error on   {'tags': []}\n",
      "Error on  Hownt , que fofo  *- {'tags': ['NOUN', 'PUNCT', 'PRON', 'VERB', 'PUNCT', 'PUNCT']}\n",
      "Error on   {'tags': []}\n",
      "Epoch  8\n",
      "Error on   {'tags': []}\n",
      "Error on   {'tags': []}\n",
      "Error on  Hownt , que fofo  *- {'tags': ['NOUN', 'PUNCT', 'PRON', 'VERB', 'PUNCT', 'PUNCT']}\n",
      "Epoch  9\n",
      "Error on   {'tags': []}\n",
      "Error on   {'tags': []}\n",
      "Error on  Hownt , que fofo  *- {'tags': ['NOUN', 'PUNCT', 'PRON', 'VERB', 'PUNCT', 'PUNCT']}\n",
      "Epoch  10\n",
      "Error on  Hownt , que fofo  *- {'tags': ['NOUN', 'PUNCT', 'PRON', 'VERB', 'PUNCT', 'PUNCT']}\n",
      "Error on   {'tags': []}\n",
      "Error on   {'tags': []}\n",
      "Epoch  11\n",
      "Error on  Hownt , que fofo  *- {'tags': ['NOUN', 'PUNCT', 'PRON', 'VERB', 'PUNCT', 'PUNCT']}\n",
      "Error on   {'tags': []}\n",
      "Error on   {'tags': []}\n",
      "Epoch  12\n",
      "Error on   {'tags': []}\n",
      "Error on   {'tags': []}\n",
      "Error on  Hownt , que fofo  *- {'tags': ['NOUN', 'PUNCT', 'PRON', 'VERB', 'PUNCT', 'PUNCT']}\n",
      "Epoch  13\n",
      "Error on   {'tags': []}\n",
      "Error on  Hownt , que fofo  *- {'tags': ['NOUN', 'PUNCT', 'PRON', 'VERB', 'PUNCT', 'PUNCT']}\n",
      "Error on   {'tags': []}\n",
      "Epoch  14\n",
      "Error on   {'tags': []}\n",
      "Error on   {'tags': []}\n",
      "Error on  Hownt , que fofo  *- {'tags': ['NOUN', 'PUNCT', 'PRON', 'VERB', 'PUNCT', 'PUNCT']}\n",
      "Epoch  15\n",
      "Error on   {'tags': []}\n",
      "Error on   {'tags': []}\n",
      "Error on  Hownt , que fofo  *- {'tags': ['NOUN', 'PUNCT', 'PRON', 'VERB', 'PUNCT', 'PUNCT']}\n",
      "Epoch  16\n",
      "Error on  Hownt , que fofo  *- {'tags': ['NOUN', 'PUNCT', 'PRON', 'VERB', 'PUNCT', 'PUNCT']}\n",
      "Error on   {'tags': []}\n",
      "Error on   {'tags': []}\n",
      "Epoch  17\n",
      "Error on   {'tags': []}\n",
      "Error on   {'tags': []}\n",
      "Error on  Hownt , que fofo  *- {'tags': ['NOUN', 'PUNCT', 'PRON', 'VERB', 'PUNCT', 'PUNCT']}\n",
      "Epoch  18\n",
      "Error on   {'tags': []}\n",
      "Error on   {'tags': []}\n",
      "Error on  Hownt , que fofo  *- {'tags': ['NOUN', 'PUNCT', 'PRON', 'VERB', 'PUNCT', 'PUNCT']}\n",
      "Epoch  19\n",
      "Error on   {'tags': []}\n",
      "Error on  Hownt , que fofo  *- {'tags': ['NOUN', 'PUNCT', 'PRON', 'VERB', 'PUNCT', 'PUNCT']}\n",
      "Error on   {'tags': []}\n"
     ]
    }
   ],
   "source": [
    "#Treinando\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "\n",
    "optimizer = nlp.begin_training()\n",
    "for i in range(20):\n",
    "    print(\"Epoch \", i)\n",
    "    random.shuffle(TRAIN_DATA)\n",
    "    for text, annotations in TRAIN_DATA:\n",
    "        try:\n",
    "            nlp.update([text], [annotations], sgd=optimizer)\n",
    "        except:\n",
    "            print(\"Error on \", text, annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.to_disk('new_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_hits = 0\n",
    "total_tags = 0\n",
    "\n",
    "for text, reference in TEST_DATA:\n",
    "    res = nlp(text)\n",
    "    i = 0\n",
    "    for token in res:\n",
    "        if token.pos_ == reference['tags'][i]:\n",
    "            tags_hits += 1\n",
    "        i += 1\n",
    "        total_tags += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102 correct tags out of 321\n"
     ]
    }
   ],
   "source": [
    "print(\"{} correct tags out of {}\".format(tags_hits, total_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
